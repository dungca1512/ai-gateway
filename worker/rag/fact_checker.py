"""
Fact Checker Module
"""
from typing import List, Dict, Any

class FactChecker:
    """
    Checks if the generated answer is supported by the provided context (grounding).
    """
    def __init__(self, llm_model: Any):
        self.llm_model = llm_model

    def check(self, answer: str, context_docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyzes the answer against the context documents to verify claims.

        Args:
            answer: The answer generated by the LLM.
            context_docs: The list of source documents used to generate the answer.

        Returns:
            A dictionary containing the analysis result.
        """
        if not self.llm_model or not hasattr(self.llm_model, 'generate'):
            print("Warning: LLM model not available for fact checking. Skipping.")
            return {"status": "skipped", "reason": "LLM not available"}

        # Combine context into a single string
        context_text = "\n\n".join([
            f"[Source {i+1}]: {doc.get('content', '')}"
            for i, doc in enumerate(context_docs)
        ])

        prompt = f"""You are a strict fact-checker. Your task is to verify if the following answer is fully supported by the provided context.

Context:
{context_text}

Answer to verify:
{answer}

Instructions:
1. Break down the answer into individual claims or sentences.
2. For each claim, determine if it is "Supported", "Partially Supported", or "Unsupported" by the context.
3. If "Unsupported" or "Partially Supported", explain why.
4. Provide a final verdict: "Fully Supported", "Partially Supported", or "Unsupported".

Output Format (JSON):
{{
    "claims": [
        {{
            "text": "claim text...",
            "status": "Supported/Partially Supported/Unsupported",
            "reason": "explanation if not supported..."
        }}
    ],
    "overall_verdict": "Fully Supported/Partially Supported/Unsupported"
}}

Return ONLY the JSON object.
"""

        messages = [
            {"role": "system", "content": "You are a helpful assistant that verifies information accuracy."},
            {"role": "user", "content": prompt}
        ]

        try:
            response = self.llm_model.generate(
                messages=messages,
                temperature=0.1, # Low temperature for consistent analysis
                max_tokens=512
            )
            content = response["choices"][0]["message"]["content"]

            # Simple parsing to handle potential markdown code blocks
            json_str = content.strip()
            if json_str.startswith("```json"):
                json_str = json_str[7:]
            if json_str.endswith("```"):
                json_str = json_str[:-3]

            import json
            try:
                analysis = json.loads(json_str)
                return analysis
            except json.JSONDecodeError:
                print(f"Error parsing fact check JSON: {content}")
                return {"status": "error", "raw_output": content}

        except Exception as e:
            print(f"Error during fact checking: {e}")
            return {"status": "error", "message": str(e)}
